{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9ERgBpbcMmB"
      },
      "source": [
        "# HW1: Frame-Level Speech Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLkH6GMGcWcE"
      },
      "source": [
        "In this homework, you will be working with MFCC data consisting of 15 features at each time step/frame. Your model should be able to recognize the phoneme occured in that frame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4vZbDmJvMp1"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI4qfx7tiBZt",
        "outputId": "e51869ed-4881-43cd-9c7d-25d9f113ce0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  mps\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchsummaryX import summary\n",
        "import sklearn\n",
        "import gc\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import datetime\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "elif torch.backends.mps.is_built():\n",
        "    device = 'mps'\n",
        "else:\n",
        "    device = 'cpu' \n",
        "\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "uniform dist px goes from 0 to 1\n",
        "px=0.25, exponential 2.5?\n",
        "no, 0.125"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "N-9qE20hmCgQ"
      },
      "outputs": [],
      "source": [
        "### PHONEME LIST\n",
        "PHONEMES = [\n",
        "            'SIL',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',  \n",
        "            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n",
        "            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n",
        "            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n",
        "            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n",
        "            'V',     'W',     'Y',     'Z',     'ZH',    '<sos>', '<eos>']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Exploration and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "datapath = '/Users/josephbajor/Dev/Datasets/11-785-f22-hw1p2/dev-clean'\n",
        "\n",
        "idx = 83\n",
        "\n",
        "mfcc_list = os.listdir(f'{datapath}/mfcc')\n",
        "trans_list = os.listdir(f'{datapath}/transcript')\n",
        "\n",
        "mfcc = np.load(f'{datapath}/mfcc/{mfcc_list[idx]}')\n",
        "trans = np.load(f'{datapath}/transcript/{mfcc_list[idx]}')[1:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "alltrans = np.concatenate([np.load(f'{datapath}/transcript/{i}') for i in trans_list])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SIL count: [396367]\n",
            "Least Common: ZH\n"
          ]
        }
      ],
      "source": [
        "unq = np.unique(alltrans, return_counts=True)\n",
        "\n",
        "#sil count\n",
        "sil_count = unq[1][np.where(unq[0]=='SIL')[0]]\n",
        "print(f'SIL count: {sil_count}')\n",
        "\n",
        "#least common phoneme\n",
        "least_common = unq[0][np.argmin(unq[1])]\n",
        "print(f'Least Common: {least_common}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([396367])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unq[1][np.where(unq[0]=='SIL')[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array(['<eos>', '<sos>', 'AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'B', 'CH',\n",
              "        'D', 'DH', 'EH', 'ER', 'EY', 'F', 'G', 'HH', 'IH', 'IY', 'JH', 'K',\n",
              "        'L', 'M', 'N', 'NG', 'OW', 'OY', 'P', 'R', 'S', 'SH', 'SIL', 'T',\n",
              "        'TH', 'UH', 'UW', 'V', 'W', 'Y', 'Z', 'ZH'], dtype='<U5'),\n",
              " array([32,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
              "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35,\n",
              "        36, 37, 38, 39, 40, 41,  1,  0]))"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.unique(PHONEMES, return_inverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Mapping tests\n",
        "phone_map = {lb:idx for idx, lb in enumerate(PHONEMES)}\n",
        "\n",
        "for idx, i in enumerate(trans):\n",
        "    trans[idx] = phone_map[i]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0, 37, 37, 37, 37, 37, 37, 37, 37,  4,  4,  4,  4,  4,\n",
              "        4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4, 28, 28, 28, 28, 20, 20,\n",
              "       20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,  3,  3,  3,\n",
              "        3,  3,  3,  3,  3,  3, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
              "       38,  3,  3,  3,  3,  3,  3, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
              "       23,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "       10, 10, 10,  3,  3,  3,  3,  3, 36, 36, 36, 36, 36, 36, 36, 36, 36,\n",
              "       36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,  6,  6,  6,\n",
              "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
              "        6,  6,  6, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
              "       21, 21, 21, 21, 21, 21,  9,  9,  9,  9, 20, 20, 20, 20, 20, 20, 20,\n",
              "       20, 20, 20,  1,  1,  1, 23, 23, 23, 23, 23, 23, 23, 23, 23, 35, 35,\n",
              "       35, 35, 25, 25, 25, 25, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
              "       21, 21, 21, 21, 21, 21, 21, 21, 35, 35, 35, 35, 37, 37, 37, 37, 37,\n",
              "       37, 37,  3,  3,  3, 21, 21, 21,  3,  3,  3,  3,  3,  3,  3, 29, 29,\n",
              "       29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
              "       29,  0,  0,  0,  0,  0,  0,  0,  0,  0, 16, 16, 16, 16, 34, 34, 34,\n",
              "       34, 34, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,  6,  6,  6,  6,  6,\n",
              "        6,  6,  6,  6,  6,  6,  6,  6,  6, 21, 21, 21, 21, 11, 11, 11, 11,\n",
              "       11, 11, 11, 11, 11, 11, 11, 11, 11, 14, 14, 14, 14, 14, 14, 14, 14,\n",
              "       14, 31, 31, 31, 31, 31, 31, 17, 17, 17, 17, 17, 23, 23, 23, 23, 23,\n",
              "       23, 10, 10, 10,  3,  3,  3, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
              "       14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
              "       18, 21, 21, 21, 21, 21,  9,  9,  9,  9,  9,  9,  9, 38, 38, 38, 38,\n",
              "       38, 38, 38, 10, 10, 10, 17, 17, 17, 17, 29, 29, 29, 29, 29, 29, 29,\n",
              "       29, 29, 22, 22, 22, 22, 22, 22, 22, 22, 22,  4,  4,  4,  4,  4,  4,\n",
              "        4,  4,  4,  4, 28, 28, 28, 23, 23, 23, 23, 23, 23, 17, 17, 17, 24,\n",
              "       24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  9,  9,  9,  9,  9,  9,  3,  3,\n",
              "        3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3, 38, 38, 38, 38, 38, 38,\n",
              "       38, 38, 38, 38, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
              "       23, 23, 23, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
              "       25, 25, 25, 25, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
              "        3,  3,  3,  3,  3,  3,  3,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
              "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8, 32, 32, 32, 32, 17, 17, 17,\n",
              "       17, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
              "       24, 24,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
              "        6,  6,  6,  3,  3,  3, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
              "       30, 30, 30, 30, 30, 30, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
              "       33, 33, 33, 28, 28, 28, 28, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
              "       37, 37, 37, 37, 37, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trans.astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## End of Testing Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vuzce0_TdcaR"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_7QgMbBdgPp"
      },
      "source": [
        "This section covers the dataset/dataloader class for speech data. You will have to spend time writing code to create this class successfully. We have given you a lot of comments guiding you on what code to write at each stage, from top to bottom of the class. Please try and take your time figuring this out, as it will immensely help in creating dataset/dataloader classes for future homeworks.\n",
        "\n",
        "Before running the following cells, please take some time to analyse the structure of data. Try loading a single MFCC and its transcipt, print out the shapes and print out the values. Do the transcripts look like phonemes?\n",
        "\n",
        "Note: Dataset functions have been modularized in another file (dataset.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNacQ8bpt9nw"
      },
      "source": [
        "# Parameters Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE7tsinAuLNy"
      },
      "source": [
        "Storing your parameters and hyperparameters in a single configuration dictionary makes it easier to keep track of them during each experiment. It can also be used with weights and biases to log your parameters for each experiment and keep track of them across multiple experiments. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PmKwlFqgt_Zq"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'epochs': 3,\n",
        "    'batch_size' : 2048,\n",
        "    'context' : 25,\n",
        "    'learning_rate' : 0.001,\n",
        "    'architecture' : 'very-low-cutoff'\n",
        "    # Add more as you need them - e.g dropout values, weight decay, scheduler parameters\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mlwaKlDt_2c"
      },
      "source": [
        "# Create Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataset import AudioDataset, AudioTestDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7xi7V8x8W9z4"
      },
      "outputs": [],
      "source": [
        "train_data = AudioDataset('/Users/josephbajor/Dev/Datasets/11-785-f22-hw1p2/train-clean-100', context=config['context']) #TODO: Create a dataset object using the AudioDataset class for the training data \n",
        "\n",
        "val_data = AudioDataset('/Users/josephbajor/Dev/Datasets/11-785-f22-hw1p2/dev-clean', context=config['context']) # TODO: Create a dataset object using the AudioDataset class for the validation data \n",
        "\n",
        "test_data = AudioTestDataset('/Users/josephbajor/Dev/Datasets/11-785-f22-hw1p2/test-clean', context=config['context']) # TODO: Create a dataset object using the AudioTestDataset class for the test data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4mzoYfTKu14s"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size:  2048\n",
            "Context:  25\n",
            "Input size:  765\n",
            "Output symbols:  42\n",
            "Train dataset samples = 36191134, batches = 17672\n",
            "Validation dataset samples = 1937496, batches = 947\n",
            "Test dataset samples = 1943303, batches = 949\n"
          ]
        }
      ],
      "source": [
        "# Define dataloaders for train, val and test datasets\n",
        "# Dataloaders will yield a batch of frames and phonemes of given batch_size at every iteration\n",
        "train_loader = torch.utils.data.DataLoader(train_data, num_workers= 1,\n",
        "                                           batch_size=config['batch_size'], pin_memory= True,\n",
        "                                           shuffle= True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(val_data, num_workers= 1,\n",
        "                                         batch_size=config['batch_size'], pin_memory= True,\n",
        "                                         shuffle= False)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_data, num_workers= 1, \n",
        "                                          batch_size=config['batch_size'], pin_memory= True, \n",
        "                                          shuffle= False)\n",
        "\n",
        "\n",
        "print(\"Batch size: \", config['batch_size'])\n",
        "print(\"Context: \", config['context'])\n",
        "print(\"Input size: \", (2*config['context']+1)*15)\n",
        "print(\"Output symbols: \", len(PHONEMES))\n",
        "\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Validation dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "n-GV3UvgLSoF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2048, 765]) torch.Size([2048])\n"
          ]
        }
      ],
      "source": [
        "# Testing code to check if your data loaders are working\n",
        "for i, data in enumerate(train_loader):\n",
        "    frames, phoneme = data\n",
        "    print(frames.shape, phoneme.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxjwve20JRJ2"
      },
      "source": [
        "# Network Architecture\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NJzT-mRw6iy"
      },
      "source": [
        "This section defines your network architecture for the homework. We have given you a sample architecture that can easily clear the very low cutoff for the early submission deadline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OvcpontXQq9j"
      },
      "outputs": [],
      "source": [
        "# This architecture will make you cross the very low cutoff\n",
        "# However, you need to run a lot of experiments to cross the medium or high cutoff\n",
        "class Network(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, context):\n",
        "\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        input_size = (2*context + 1) * 15 #Why is this the case? Because we flattened the entire MFCC\n",
        "        output_size = 40 #Why? One hot encoded\n",
        "\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_size, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.model(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HejoSXe3vMVU"
      },
      "source": [
        "# Define Model, Loss Function and Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAhGBH7-xxth"
      },
      "source": [
        "Here we define the model, loss function, optimizer and optionally a learning rate scheduler. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_qtrEM1ZvLje"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=============================================================\n",
            "                 Kernel Shape Output Shape  Params  Mult-Adds\n",
            "Layer                                                        \n",
            "0_model.Linear_0    [765, 40]   [2048, 40]   30640      30600\n",
            "-------------------------------------------------------------\n",
            "                      Totals\n",
            "Total params           30640\n",
            "Trainable params       30640\n",
            "Non-trainable params       0\n",
            "Mult-Adds              30600\n",
            "=============================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_model.Linear_0</th>\n",
              "      <td>[765, 40]</td>\n",
              "      <td>[2048, 40]</td>\n",
              "      <td>30640</td>\n",
              "      <td>30600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Kernel Shape Output Shape  Params  Mult-Adds\n",
              "Layer                                                        \n",
              "0_model.Linear_0    [765, 40]   [2048, 40]   30640      30600"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_size = 15*(2*config['context'] + 1)\n",
        "model = Network(config['context']).to(device)\n",
        "frames,phoneme = next(iter(train_loader))\n",
        "# Check number of parameters of your network - Remember, you are limited to 20 million parameters for HW1 (including ensembles)\n",
        "summary(model, frames.to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "30640"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UROGEVJevKD-"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss() #Defining Loss function \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate']) #Defining Optimizer\n",
        "# Recommended : Define Scheduler for Learning Rate, including but not limited to StepLR, MultiStepLR, CosineAnnealingLR, ReduceLROnPlateau, etc. \n",
        "# You can refer to Pytorch documentation for more information on how to use them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBwunYpyugFg"
      },
      "source": [
        "# Training and Validation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JgeNhx4x2-P"
      },
      "source": [
        "This section covers the training, and validation functions for each epoch of running your experiment with a given model architecture. The code has been provided to you, but we recommend going through the comments to understand the workflow to enable you to write these loops for future HWs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XblOHEVtKab2",
        "outputId": "e7a3c98e-8429-46a4-f348-ac4e21bd03ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if device == 'cuda':\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8wjPz7DHqKcL"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, criterion, dataloader):\n",
        "\n",
        "    model.train()\n",
        "    train_loss = 0.0 #Monitoring Loss\n",
        "    \n",
        "    for iter, (mfccs, phonemes) in enumerate(dataloader):\n",
        "\n",
        "        ### Initialize Gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        ### Move Data to Device (Ideally GPU)\n",
        "        mfccs = mfccs.to(device)\n",
        "        phonemes = phonemes.to(device)\n",
        "\n",
        "        ### Forward Propagation\n",
        "        logits = model(mfccs)\n",
        "\n",
        "        ### Loss Calculation\n",
        "        loss = criterion(logits, phonemes)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        ### Backward Propagation\n",
        "        loss.backward()\n",
        "\n",
        "        ### Gradient Descent\n",
        "        optimizer.step()\n",
        "  \n",
        "    train_loss /= len(dataloader)\n",
        "    return train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Q5npQNFH315V"
      },
      "outputs": [],
      "source": [
        "def eval(model, dataloader):\n",
        "\n",
        "    model.eval() # set model in evaluation mode\n",
        "\n",
        "    phone_true_list = []\n",
        "    phone_pred_list = []\n",
        "\n",
        "    for i, data in enumerate(dataloader):\n",
        "\n",
        "        frames, phonemes = data\n",
        "        ### Move data to device (ideally GPU)\n",
        "        frames, phonemes = frames.to(device), phonemes.to(device) \n",
        "\n",
        "        with torch.inference_mode(): # makes sure that there are no gradients computed as we are not training the model now\n",
        "            ### Forward Propagation\n",
        "            logits = model(frames)\n",
        "\n",
        "        ### Get Predictions\n",
        "        predicted_phonemes = torch.argmax(logits, dim=1)\n",
        "        \n",
        "        ### Store Pred and True Labels\n",
        "        phone_pred_list.extend(predicted_phonemes.cpu().tolist())\n",
        "        phone_true_list.extend(phonemes.cpu().tolist())\n",
        "        \n",
        "        # Do you think we need loss.backward() and optimizer.step() here?\n",
        "    \n",
        "        del frames, phonemes, logits\n",
        "        if device == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    ### Calculate Accuracy\n",
        "    accuracy = accuracy_score(phone_pred_list, phone_true_list) \n",
        "    return accuracy*100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMd_XxPku5qp"
      },
      "source": [
        "# Weights and Biases Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjIbhR1wwbgI"
      },
      "source": [
        "This section is to enable logging metrics and files with Weights and Biases. Please refer to wandb documentationa and recitation 0 that covers the use of weights and biases for logging, hyperparameter tuning and monitoring your runs for your homeworks. Using this tool makes it very easy to show results when submitting your code and models for homeworks, and also extremely useful for study groups to organize and run ablations under a single team in wandb. \n",
        "\n",
        "We have written code for you to make use of it out of the box, so that you start using wandb for all your HWs from the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "SCDYx5VEu6qI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjbajor\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/josephbajor/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.13.3 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/josephbajor/Library/CloudStorage/OneDrive-Personal/Code/CMU/Deep Learning/HW1/wandb/run-20220913_220400-32g0c24g</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/jbajor/IDL-hw1p2/runs/32g0c24g\" target=\"_blank\">test2</a></strong> to <a href=\"https://wandb.ai/jbajor/IDL-hw1p2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "['/Users/josephbajor/Library/CloudStorage/OneDrive-Personal/Code/CMU/Deep Learning/HW1/wandb/run-20220913_220400-32g0c24g/files/model_arch.txt']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login(key=\"c319fb8dfa7ce22e07aa0cefe0823a9752d50720\") #API Key is in your wandb account, under settings (wandb.ai/settings)\n",
        "\n",
        "### Create your wandb run\n",
        "run = wandb.init(\n",
        "    name = \"test2\", ### Wandb creates random run names if you skip this field, we recommend you give useful names\n",
        "    reinit=True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    project=\"IDL-hw1p2\", ### Project should be created in your wandb account \n",
        "    config=config ### Wandb Config for your run\n",
        ")\n",
        "### Save your model architecture as a string with str(model) \n",
        "model_arch = str(model)\n",
        "\n",
        "### Save it in a txt file \n",
        "arch_file = open(\"model_arch.txt\", \"w\")\n",
        "file_write = arch_file.write(model_arch)\n",
        "arch_file.close()\n",
        "\n",
        "### log it in your wandb run with wandb.save()\n",
        "wandb.save('model_arch.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nclx_04fu7Dd"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "36191134"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdLMWfEpyGOB"
      },
      "source": [
        "Now, it is time to finally run your ablations! Have fun!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "MG4F77Nm0Am9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/3\n",
            "\tTrain Loss: 1.6417\n",
            "\tValidation Accuracy: 52.31%\n",
            "\n",
            "Epoch 2/3\n",
            "\tTrain Loss: 1.5751\n",
            "\tValidation Accuracy: 52.41%\n",
            "\n",
            "Epoch 3/3\n",
            "\tTrain Loss: 1.5711\n",
            "\tValidation Accuracy: 52.34%\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2555009e706446d6bff9fdb4310b6d29",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train loss</td><td>█▁▁</td></tr><tr><td>validation accuracy</td><td>▁█▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train loss</td><td>1.5711</td></tr><tr><td>validation accuracy</td><td>52.34246</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">test2</strong>: <a href=\"https://wandb.ai/jbajor/IDL-hw1p2/runs/32g0c24g\" target=\"_blank\">https://wandb.ai/jbajor/IDL-hw1p2/runs/32g0c24g</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220913_220400-32g0c24g/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Iterate over number of epochs to train and evaluate your model\n",
        "if device == 'cuda':\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "best_acc = 0.0 ### Monitor best accuracy in your run\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n",
        "\n",
        "    train_loss = train(model, optimizer, criterion, train_loader)\n",
        "    accuracy = eval(model, val_loader)\n",
        "\n",
        "    print(\"\\tTrain Loss: {:.4f}\".format(train_loss))\n",
        "    print(\"\\tValidation Accuracy: {:.2f}%\".format(accuracy))\n",
        "\n",
        "\n",
        "    ### Log metrics at each epoch in your run - Optionally, you can log at each batch inside train/eval functions (explore wandb documentation/wandb recitation)\n",
        "    wandb.log({\"train loss\": train_loss, \"validation accuracy\": accuracy})\n",
        "\n",
        "    ### Save checkpoint if accuracy is better than your current best\n",
        "    if accuracy >= best_acc:\n",
        "\n",
        "      ### Save checkpoint with information you want\n",
        "      torch.save({'epoch': epoch,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'loss': train_loss,\n",
        "              'acc': accuracy},\n",
        "        './model_checkpoint.pth')\n",
        "      \n",
        "      ### Save checkpoint in wandb\n",
        "      wandb.save('checkpoint.pth')\n",
        "\n",
        "    # Is your training time very high? Look into mixed precision training if your GPU (Tesla T4, V100, etc) can make use of it \n",
        "    # Refer - https://pytorch.org/docs/stable/notes/amp_examples.html\n",
        "\n",
        "### Finish your wandb run\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kXwf5YUo_4A"
      },
      "source": [
        "# Testing and submission to Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI1hSFYLpJvH"
      },
      "source": [
        "Before we get to the following code, make sure to see the format of submission given in *random_submission.csv*. Once you have done so, it is time to fill the following function to complete your inference on test data. Refer the eval function from previous cells to get an idea of how to go about completing this function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "R-SU9fZ3xHtk"
      },
      "outputs": [],
      "source": [
        "def test(model, test_loader):\n",
        "  ### What you call for model to perform inference?\n",
        "  model.____\n",
        "\n",
        "  ### List to store predicted phonemes of test data\n",
        "  test_predictions = []\n",
        "\n",
        "  ### Which mode do you need to avoid gradients?\n",
        "  with torch._____:\n",
        "\n",
        "      for i, frames in enumerate(tqdm(test_loader)):\n",
        "\n",
        "          frames = frames.float().to(device)             \n",
        "          \n",
        "          output = model(frames)\n",
        "\n",
        "          ### Get most likely predicted phoneme with argmax\n",
        "          predicted_phonemes = NotImplemented\n",
        "\n",
        "          ### How do you store predicted_phonemes with test_predictions? Hint, look at eval \n",
        "          \n",
        "  return test_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "wG9v6Xmxu7wp"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'Network' object has no attribute '____'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/Users/josephbajor/Library/CloudStorage/OneDrive-Personal/Code/CMU/Deep Learning/HW1/HW1P2_Starter_Student.ipynb Cell 48\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/josephbajor/Library/CloudStorage/OneDrive-Personal/Code/CMU/Deep%20Learning/HW1/HW1P2_Starter_Student.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m predictions \u001b[39m=\u001b[39m test(model, test_loader)\n",
            "\u001b[1;32m/Users/josephbajor/Library/CloudStorage/OneDrive-Personal/Code/CMU/Deep Learning/HW1/HW1P2_Starter_Student.ipynb Cell 48\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, test_loader)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephbajor/Library/CloudStorage/OneDrive-Personal/Code/CMU/Deep%20Learning/HW1/HW1P2_Starter_Student.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest\u001b[39m(model, test_loader):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephbajor/Library/CloudStorage/OneDrive-Personal/Code/CMU/Deep%20Learning/HW1/HW1P2_Starter_Student.ipynb#X63sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m   \u001b[39m### What you call for model to perform inference?\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/josephbajor/Library/CloudStorage/OneDrive-Personal/Code/CMU/Deep%20Learning/HW1/HW1P2_Starter_Student.ipynb#X63sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m   model\u001b[39m.\u001b[39;49m____\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephbajor/Library/CloudStorage/OneDrive-Personal/Code/CMU/Deep%20Learning/HW1/HW1P2_Starter_Student.ipynb#X63sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m   \u001b[39m### List to store predicted phonemes of test data\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephbajor/Library/CloudStorage/OneDrive-Personal/Code/CMU/Deep%20Learning/HW1/HW1P2_Starter_Student.ipynb#X63sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m   test_predictions \u001b[39m=\u001b[39m []\n",
            "File \u001b[0;32m~/mambaforge/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1206\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1207\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1208\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Network' object has no attribute '____'"
          ]
        }
      ],
      "source": [
        "predictions = test(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZE1hRnvf0bFz"
      },
      "outputs": [],
      "source": [
        "### Create CSV file with predictions\n",
        "with open(\"./submission.csv\", \"w+\") as f:\n",
        "    f.write(\"id,label\\n\")\n",
        "    for i in range(len(predictions)):\n",
        "        f.write(\"{},{}\\n\".format(i, predictions[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjcammuCxMKN"
      },
      "outputs": [],
      "source": [
        "### Submit to kaggle competition using kaggle API\n",
        "!kaggle competitions submit -c 11-785-f22-hw1p2 -f ./submission.csv -m \"Test Submission\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "z4vZbDmJvMp1",
        "ZIi0Big7vPa9",
        "Vuzce0_TdcaR",
        "qNacQ8bpt9nw",
        "2mlwaKlDt_2c",
        "Nxjwve20JRJ2",
        "HejoSXe3vMVU",
        "IBwunYpyugFg",
        "yMd_XxPku5qp",
        "nclx_04fu7Dd",
        "_kXwf5YUo_4A"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.6 ('dl')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "53c39f520a13645dd7c4ca3fb85bdd1c8dd8e9c5229bf594fcee21a38b2fab6d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
